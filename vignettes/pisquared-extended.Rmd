---
title: "pi-squared-example"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{pi-squared-example}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE
)
library(pisquared)
require(tidyverse)
require(magrittr)
require(rstan)
require(doMC)
registerDoMC(detectCores() - 1) 
theme_set(theme_bw(base_size = 14))
```

## Model-based estimation of marginal pi0

Let's check how good a job we can do of estimating marginal pi0 across a range of parameters. We'll simulate some p-values with specified non-null rates. We'll use Beta(a=0.2,b=2) for the non-null pvalues: 
```{r test-marginal}
true_alpha = 0.2
true_beta = 2
N = 1000
marg_results = foreach(true_pi1 = c(0.001,0.01,0.05,seq(.1,1,by=0.1)), .combine = bind_rows) %:%
  foreach(rep = 1:30, .combine = bind_rows) %dopar% {
    p = simulate_pvalues(N = N, pi1 = true_pi1, alpha = true_alpha, beta = true_beta)
    res = tryCatch(
      model_based_pi0(p, method = "optimizing") %>% 
        as_tibble() %>% 
        mutate(true_pi1 = true_pi1, 
               pi1 = 1 - pi0, 
               rep = rep, 
               qvalue_pi1 = 1 - my_pi0est(p)$pi0),
      error = function(e) NULL
    )
}
marg_results
```

Both Storey's qvalue and our model based approach give approximately unbiased estimates of pi0 in this setting: 
```{r vary-true-pi0, fig.width=4.5, fig.height=4}
res_gathered = marg_results %>% select(true_pi1, model_based = pi1, qvalue = qvalue_pi1) %>% 
  gather(method, est_pi1, -true_pi1) 
res_gathered  %>% ggplot(aes(true_pi1, est_pi1, col = method, shape = method)) + geom_point(alpha = 0.5, size = 2, position = position_dodge(width = 0.02)) + 
  geom_abline(intercept = 0, slope = 1) + xlab("True pi1") + ylab("Estimated pi1") + theme(legend.position = c(0.25,.8))
```

Errors are substantially lower for pisquared than Storey's pi0, as expected since the data is generated from the pisquared model. Additionally Storey's pi0 can be negative which is clearly unreasonable. 
```{r rmse-comparison, fig.width=5, fig.height=4}
err_summary = res_gathered %>% 
  group_by(method, true_pi1) %>% 
  summarize(RMSE = sqrt(mean((true_pi1 - est_pi1)^2))) %>% 
  ungroup()
err_summary %>% ggplot(aes(true_pi1, RMSE, col = method, shape = method)) + geom_point(size = 2) + expand_limits(y = 0) + theme(legend.position = c(0.8,.8))
```

alpha and beta are estimated quite robustly: 
```{r alpha-beta-estimation, fig.width=5, fig.height=4}
marg_results %>% 
  ggplot(aes(true_pi1, alpha)) + geom_point(alpha = 0.3, size = 3) + geom_hline(yintercept = true_alpha) + xlab("True pi1") + ylab("Estimated alpha")
marg_results %>% 
  ggplot(aes(true_pi1, beta)) + geom_point(alpha = 0.3, size = 3) + geom_hline(yintercept = true_beta) + xlab("True pi1") + ylab("Estimated beta")
```


```{r test-marginal-mix}
true_alpha = 0.2
true_beta = 2
N = 1000
marg_results = foreach(true_pi1 = c(0.001,0.01,0.05,seq(.1,1,by=0.1)), .combine = bind_rows) %:%
  foreach(rep = 1:30, .combine = bind_rows) %dopar% {
    p = simulate_pvalues(N = N, pi1 = true_pi1, alpha = true_alpha, beta = true_beta)
    mm = model_based_pi0_mix(p)
        tibble(true_pi1 = true_pi1, 
               pi1 = 1 - mm$par$pi0, 
               rep = rep, 
               qvalue_pi1 = 1 - my_pi0est(p)$pi0)
}
marg_results
```

```{r vary-true-pi0, fig.width=4.5, fig.height=4}
res_gathered = marg_results %>% select(true_pi1, model_based = pi1, qvalue = qvalue_pi1) %>% 
  gather(method, est_pi1, -true_pi1) 
res_gathered %>% ggplot(aes(true_pi1, est_pi1, col = method, shape = method)) + geom_point(alpha = 0.5, size = 2, position = position_dodge(width = 0.02)) + 
  geom_abline(intercept = 0, slope = 1) + xlab("True pi1") + ylab("Estimated pi1") + theme(legend.position = c(0.25,.8))
```

## Estimating sharing

We first consider independent p-values but with different non-null rates: 
```{r independent-p, fig.width=5, fig.height=4}
true_pi1s = c(0.1, 0.5)
p1 = simulate_pvalues(N = N, pi1 = true_pi1s[1], alpha = true_alpha, beta = true_beta)
p2 = simulate_pvalues(N = N, pi1 = true_pi1s[2], alpha = true_alpha, beta = true_beta)
hist(p1, 50)
hist(p2, 50)
```

In our simulation p1 and p2 are independent so the "true" Jaccard index is small:
```{r true-jaccard}
true_jaccard = true_pi1s[1] * true_pi1s[2] / (1 - (1 - true_pi1s[1]) * (1 - true_pi1s[2]))
true_jaccard
```

pisquared can use three methods provided by stan: maximum likelihood estimation "optimizing", variational Bayes ("vb") and Markov chain Monte Carlo ("sampling"). The fastest fitting approach is to do maximum likelihood estimation via LBFGS. 
```{r compare-methods, message=F, echo=F}
methods = c("optimizing", "vb", "sampling")
results = foreach(method = methods, .combine = bind_rows) %do% {
  foreach(bin_pvalues = c(T,F), .combine = bind_rows) %dopar% { 
    cpu_time = system.time( 
      pi2_result <- pi2_estimator(p1,
                               p2,
                               method = method, 
                               bin_pvalues = bin_pvalues)
    )
    marginal_pi0 = c(rowSums(pi2_result$pi)[1], colSums(pi2_result$pi)[1])
    tibble( method = method, 
            bin_pvalues = bin_pvalues,
            jaccard = pi2_result$jaccard, 
            jaccard_sd = pi2_result$jaccard_sd, 
            pi0_1 = marginal_pi0[1],
            pi0_2 = marginal_pi0[2], 
            cpu_time = cpu_time[1])
  }
}
results
```

The red line here shows the true Jaccard sharing index. binning pvalues does come at a cost in accuracy. 
```{r compare-methods-accuracy, fig.width=5, fig.height=4}
results %>% ggplot(aes(method, jaccard, 
                       ymin = jaccard - jaccard_sd, 
                       ymax = jaccard + jaccard_sd, 
                       fill = bin_pvalues)) + 
  geom_col(position = "dodge") + 
  geom_errorbar(position = "dodge") +
  geom_hline(yintercept = true_jaccard, 
             col = "red")
```

The computation time for these methods is very different:
```{r compare-methods-time, fig.width=5, fig.height=4}
results %>% mutate(method = factor(method, methods)) %>% 
  ggplot(aes(method, 1000 * cpu_time, fill = bin_pvalues)) + 
  geom_col(position = "dodge") + scale_y_log10() + ylab("Computation time (ms)")
```

Optimization performs as well as VB or sampling and is an order of magnitude (or two) faster so we'll use it for the rest of the vignette:
```{r use-optimziation}
pi2_result = pi2_estimator(p1,
                           p2,
                           method = "optimizing")
marginal_pi1 = c(rowSums(pi2_result$pi)[2], colSums(pi2_result$pi)[2])
marginal_pi1
```

Plot fitted marginal p-value distributions (estimated vs true)
```{r check-marginal-distribution, fig.width=5, fig.height=4}
dpi0 = function(p, pi1, alpha, beta) {
  (1-pi1)  + pi1 * dbeta(p, alpha, beta)
}
for (i in 1:2) { 
  plot(function(g) dpi0(g, true_pi1s[i], true_alpha, true_beta), 
       ylab = "non-null p-val dist", 
       ylim = c(0,2),
       col = 1)
  abline(h = 1-true_pi1s[i], col = 1, lty = 2)
  plot(function(g) dpi0(g, marginal_pi1[i], pi2_result$fit$par$alpha[i], pi2_result$fit$par$beta[i]), 
       col = 2,
       add = T)
  abline(h = 1-marginal_pi1[i], col = 2, lty = 2)
  legend("topright", legend=c("True", "Estimated"), col=1:2, lty = 1)
  grid()
}
```

## Estimating sharing across a range of underlying sharing levels

Here we simulation two arrays of p-values with specific sharing ("true_jaccard") and 
```{r vary-sharing}
N = 10000
jacc_results = foreach(true_jaccard = seq(0,1,by=.1), .combine = bind_rows) %:% 
  foreach(rep = 1:10, .combine = bind_rows) %dopar% {  # .combine = bind_rows, 
    set.seed(rep)
    dat = simulate_joint_pvalues(N = N, 
                                pi_1 = 0.1, 
                                pi_2 = 0.1,
                                true_jaccard = true_jaccard, 
                                alpha = true_alpha, 
                                beta = true_beta)
    if (is.null(dat)) return(NULL)
    counts = dat %>% select(hit1, hit2) %>% table()
    pi2_result <- pi2_estimator(dat$p1, dat$p2) # , method = "vb")
    tibble(true_jaccard = true_jaccard, 
           actual_jacc = counts["TRUE", "TRUE"] / (counts["FALSE", "TRUE"] +  counts["TRUE", "FALSE"] + counts["TRUE", "TRUE"]), 
           est_pi1_1 = rowSums(pi2_result$pi)[1], 
           est_pi1_2 = colSums(pi2_result$pi)[1],
           est_jaccard = pi2_result$jaccard, 
           jaccard_sd = pi2_result$jaccard_sd)
}
jacc_results
```

Plot the results: we see pisquared provides unbiased and accurate sharing estimates. 
```{r true-v-est-jaccard, fig.width=5, fig.height=4}
jacc_results %>% ggplot(aes(actual_jacc, est_jaccard)) + geom_point() + xlab("Actual sharing (Jaccard)") + ylab("Estimated sharing (Jaccard)") + geom_abline(intercept = 0, slope = 1)
```

